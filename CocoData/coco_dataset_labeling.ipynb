{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T11:22:54.390534Z",
     "start_time": "2025-04-04T11:22:54.386412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:22:54.689750Z",
     "start_time": "2025-04-04T11:22:54.686697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#train2017\n",
    "#val2017\n",
    "data_dir = 'val2017'\n",
    "panoptic_label_dir = 'panoptic_val2017'\n",
    "json_file = 'panoptic_val2017.json'\n",
    "files_to_check = [panoptic_label_dir, json_file]"
   ],
   "id": "ac7b5ada4a9d90bc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:22:55.036017Z",
     "start_time": "2025-04-04T11:22:55.032720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_panoptic_label_dir_name = panoptic_label_dir + '_' + 'edited'\n",
    "save_panoptic_data_dir_name = data_dir + '_' + 'edited'\n",
    "save_dir = 'val_coco_dataset_ex'\n",
    "save_panoptic_label_dir_path = (save_dir + '/' + save_panoptic_label_dir_name)\n",
    "save_panoptic_data_dir_path = (save_dir + '/' + save_panoptic_data_dir_name)"
   ],
   "id": "e759ed53e48b4dd5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:23:34.490183Z",
     "start_time": "2025-04-04T11:23:34.486171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(save_panoptic_label_dir_path)\n",
    "print(save_panoptic_data_dir_path)"
   ],
   "id": "8b94dcbd3646491c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_coco_dataset_ex/panoptic_val2017_edited\n",
      "val_coco_dataset_ex/val2017_edited\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:23:34.844158Z",
     "start_time": "2025-04-04T11:23:34.838729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(save_panoptic_label_dir_path, exist_ok=True)\n",
    "os.makedirs(save_panoptic_data_dir_path, exist_ok=True)"
   ],
   "id": "5cd530ff7d626f98",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:23:35.406381Z",
     "start_time": "2025-04-04T11:23:35.401843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_file_endings(files_to_check, expected_ending):\n",
    "    for file_path in files_to_check:\n",
    "        if os.path.isfile(file_path):\n",
    "            result = re.search(r'_([^.]+)', os.path.basename(file_path)).group(1)\n",
    "            if result != expected_ending:\n",
    "                raise AssertionError(f\"Error: File '{file_path}' has ending '{result}', but expected '{expected_ending}'.\")\n",
    "        else:\n",
    "            if not file_path.endswith(expected_ending):\n",
    "                actual_ending = os.path.splitext(file_path)[-1]\n",
    "                raise AssertionError(f\"Error: File '{file_path}' does not end with '{expected_ending}', found ending '{actual_ending}' instead.\")\n"
   ],
   "id": "3318ca1a5d514410",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:23:35.872036Z",
     "start_time": "2025-04-04T11:23:35.867476Z"
    }
   },
   "cell_type": "code",
   "source": "check_file_endings(files_to_check, data_dir)",
   "id": "79b93503a9b7b70b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:23:36.419012Z",
     "start_time": "2025-04-04T11:23:36.414967Z"
    }
   },
   "cell_type": "code",
   "source": "annotations_count = []",
   "id": "53c0f86bb13b4e3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:23:37.164072Z",
     "start_time": "2025-04-04T11:23:36.992008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(json_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "for annotation in data['annotations']:\n",
    "    annotations_count.append(annotation['file_name'])"
   ],
   "id": "ae38a8dfb3e51b71",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:23:37.594028Z",
     "start_time": "2025-04-04T11:23:37.588399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_region(zeros_slice, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    region = zeros_slice[y:y+h, x:x+w]\n",
    "    mask_zeros = zeros_slice == 255\n",
    "    mask_region = region == 255\n",
    "    return np.sum(mask_zeros) == np.sum(mask_region)\n",
    "\n",
    "# check_sizes(non_human_sizes, human_sizes, non_human_masks, human_masks , mask_layers[i])\n",
    "def check_sizes(no_h_sizes, h_sizes, no_h_memory, h_memory, mask_layer_i):\n",
    "    # [obj1_size, obj2_size, ....]\n",
    "    filtered_no_h_iou = [ np.sum(mask_layer_i == 255)/ no_h_sizes[j] for j in range(len(no_h_memory)) if no_h_memory[j]]\n",
    "    # [hum1_size, hum2_size, .....]\n",
    "    filtered_h_iou = [ np.sum(mask_layer_i == 255)/ h_sizes[j] for j in range(len(h_memory)) if h_memory[j]]\n",
    "\n",
    "    max_no_h_iou = 0\n",
    "    max_h_iou = 0\n",
    "\n",
    "    # maximal ious comparison\n",
    "    for no_h_size in filtered_no_h_iou:\n",
    "        if no_h_size>max_no_h_iou:\n",
    "            max_no_h_iou = no_h_size\n",
    "    for h_size in filtered_h_iou:\n",
    "        if h_size>max_h_iou:\n",
    "            max_h_iou = h_size\n",
    "\n",
    "    if max_no_h_iou > max_h_iou:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ],
   "id": "16e70026486b2e73",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:23:38.398738Z",
     "start_time": "2025-04-04T11:23:38.370947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(annotations_count))\n",
    "print(len(os.listdir(data_dir)))\n",
    "print(len(os.listdir(panoptic_label_dir)))"
   ],
   "id": "a071b6f50381958",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-04T11:23:39.435286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_segments(annotations, filename):\n",
    "    human_bbox = []\n",
    "    no_human_bbox = []\n",
    "    obj_bbox = []\n",
    "    for annotation in annotations:\n",
    "        if annotation['file_name'] == filename:\n",
    "            for segment in annotation['segments_info']:\n",
    "                if segment['category_id'] == 1:\n",
    "                    human_bbox.append(segment)\n",
    "                elif 1 < segment['category_id'] <= 90:\n",
    "                    obj_bbox.append(segment)\n",
    "                elif segment['category_id'] >= 92:\n",
    "                    no_human_bbox.append(segment)\n",
    "    return human_bbox, no_human_bbox, obj_bbox\n",
    "\n",
    "def process_file(filename):\n",
    "    human_bbox, no_human_bbox, obj_bbox = extract_segments(data['annotations'], filename)\n",
    "    segments = {\n",
    "        1: human_bbox,  # Human segments\n",
    "        2: no_human_bbox,  # Non-human segments\n",
    "        3: obj_bbox,  # Object segments\n",
    "    }\n",
    "\n",
    "\n",
    "    im = cv2.imread(os.path.join(panoptic_label_dir, filename))\n",
    "    height, width, _ = im.shape\n",
    "    unique_colors = np.unique(im.reshape(-1, 3), axis=0)\n",
    "    mask_layers = np.zeros((len(unique_colors), height, width), dtype=np.uint8)\n",
    "\n",
    "    for i, color in enumerate(unique_colors):\n",
    "        mask = (im == color).all(axis=2)\n",
    "        mask_layers[i] = mask.astype(np.uint8) * 255\n",
    "\n",
    "        human_sizes = np.array([s['bbox'][2] * s['bbox'][3] for s in segments[1]])\n",
    "        non_human_sizes = np.array([s['bbox'][2] * s['bbox'][3] for s in segments[2]])\n",
    "        object_sizes = np.array([s['bbox'][2] * s['bbox'][3] for s in segments[3]])\n",
    "        human_masks = np.array([check_region(mask_layers[i], s['bbox']) for s in segments[1]])\n",
    "        non_human_masks = np.array([check_region(mask_layers[i], s['bbox']) for s in segments[2]])\n",
    "        object_masks = np.array([check_region(mask_layers[i], s['bbox']) for s in segments[3]])\n",
    "\n",
    "        if not np.any(human_masks):\n",
    "            mask_layers[i] = np.zeros_like(mask_layers[i])\n",
    "        elif np.any(human_masks) and np.any(non_human_masks) and check_sizes(non_human_sizes, human_sizes, non_human_masks, human_masks , mask_layers[i]):\n",
    "            mask_layers[i] = np.zeros_like(mask_layers[i])\n",
    "        elif np.any(human_masks) and np.any(object_masks) and check_sizes(object_sizes, human_sizes, object_masks, human_masks, mask_layers[i]):\n",
    "            mask_layers[i] = np.zeros_like(mask_layers[i])\n",
    "\n",
    "    overlay_sum = np.sum(mask_layers, axis=0)\n",
    "    if np.all(overlay_sum == 0):\n",
    "        return\n",
    "    base_name, ext = os.path.splitext(filename)\n",
    "    cv2.imwrite(f\"{save_panoptic_label_dir_path}/{filename}\", overlay_sum)\n",
    "    shutil.copy(f'{data_dir}/{base_name}.jpg', f\"{save_panoptic_data_dir_path}/{base_name}.jpg\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    filenames = os.listdir(panoptic_label_dir)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        list(tqdm(executor.map(process_file, filenames), desc=\"Processing files\", unit=\"file\", total=len(filenames)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "ef0a4e2cd0aa7d7d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%|‚ñè         | 77/5000 [00:21<22:50,  3.59file/s]  \n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9797d88bd28a137c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e552ff7882c9372d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
